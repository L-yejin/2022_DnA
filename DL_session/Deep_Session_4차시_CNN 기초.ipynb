{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8430ce41",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\">Deep_Session_4차시_CNN 기초</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95fc9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f082c",
   "metadata": {},
   "source": [
    "- [Convolution의 output 크기](#Convolution의-output-크기-실습)\n",
    "- [CNN 실습](#CNN-실습)\n",
    "- [Data Augmentation 기법](#Data-Augmentation-기법-이용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f7df6",
   "metadata": {},
   "source": [
    "# Convolution의 output 크기 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af0cc97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d(1,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b021df5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(5, 10), stride=(1, 2), padding=(2, 4))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d(1,1,(5,10), stride=(1,2), padding=(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e11dc7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 1, kernel_size=(11, 11), stride=(4, 4))\n",
      "torch.Size([1, 1, 227, 227])\n",
      "torch.Size([1, 1, 55, 55])\n"
     ]
    }
   ],
   "source": [
    "# 예제 1\n",
    "conv = nn.Conv2d(1,1,11, stride=4, padding=0)\n",
    "print(conv)\n",
    "inputs = torch.Tensor(1,1,227,227)\n",
    "print(inputs.shape)\n",
    "out = conv(inputs)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34ece988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# 예제 2\n",
    "conv = nn.Conv2d(1,1,5, stride=1, padding=2)\n",
    "print(conv)\n",
    "inputs = torch.Tensor(1,1,32,32)\n",
    "print(inputs.shape)\n",
    "out = conv(inputs)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6086e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([1, 1, 64, 32])\n",
      "torch.Size([1, 1, 64, 32])\n"
     ]
    }
   ],
   "source": [
    "# 예제 3\n",
    "conv = nn.Conv2d(1,1,3, stride=1, padding=1)\n",
    "print(conv)\n",
    "inputs = torch.Tensor(1,1,64,32)\n",
    "print(inputs.shape)\n",
    "out = conv(inputs)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde653a",
   "metadata": {},
   "source": [
    "# CNN 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dccf534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module import하기\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b32fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.11.0  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 장비 확인하기\n",
    "# DEVICE 설정\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01245f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 내 하이퍼파리미터\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f0ec100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR10 데이터 다운로드(Train set, Test set 분리하기)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root = \"../data/CIFAR_10\",\n",
    "                                train = True,\n",
    "                                download = True,\n",
    "                                transform = transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root = \"../data/CIFAR_10\",\n",
    "                                train = False,\n",
    "                                transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                          shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                         batch_size = BATCH_SIZE,\n",
    "                                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d41c6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 3, 32, 32]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인 1\n",
    "\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9316cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 확인 2\n",
    "\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(X_train[i], (1, 2, 0)))\n",
    "    plt.title('Class: ' + str(y_train[i].item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1cadb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 설계하기\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(   # Conv layer1\n",
    "        in_channels = 3,\n",
    "        out_channels = 8,\n",
    "        kernel_size = 3,\n",
    "        padding = 1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(   # Conv layer2\n",
    "        in_channels = 8,\n",
    "        out_channels = 16,\n",
    "        kernel_size = 3,\n",
    "        padding = 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d( # Pooling layer\n",
    "        kernel_size = 2,\n",
    "        stride = 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * 16, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 8 * 8 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c2348f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Optimizer, Objective Function 설정하기\n",
    "\n",
    "model = CNN().to(DEVICE)                                      # 디바이스 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ed152",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd27a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP와 비교\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):                     # MLP클래스의 인스턴수를 생성했을 때 갖게 되는 성질을 정의하는 메소드\n",
    "        super(MLP, self).__init__()         # nn.Module 내에 있는 메소드 상속받아 이용\n",
    "        \n",
    "        self.fc1 = nn.Linear(32*32*3, 512)  # 첫번째 fc layer 정의\n",
    "        self.fc2 = nn.Linear(512, 256)      # 두번째 fc layer 정의\n",
    "        self.fc3 = nn.Linear(256, 10)       # 세번째 fc layer 정의\n",
    "\n",
    "        \n",
    "    def forward(self, x):                   # 설계한 MLP모델에 데이터를 입력했을 때, Output까지의 계산 과정을 나열한 것\n",
    "        x = x.view(-1, 32*32*3)             # MLP모델은 1차원 벡터값을 입력으로 받을 수 있기 때문에 (32*32*3)크기의 2차원 데이터를 view로 1차운으로 변환\n",
    "        x = self.fc1(x)                     # fc layer1 통과\n",
    "        x = F.relu(x)                       # 비선형 함수 ReLU() 계산 -> fc layer2의 input으로 계산\n",
    "        x = self.fc2(x)                     # fc layer2 통과\n",
    "        x = F.relu(x)                       # 비선형 함수 ReLU() 계산\n",
    "        x = self.fc3(x)                     # fc layer3 통과\n",
    "        x = F.log_softmax(x, dim=1)         # 10가지의 경우의 수 중 하나로 분류 (log_softmax() 사용 이유 : Back Propagation을 이용해 학습이 더 원활하게 진행하기 위해)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a99a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이어를 쌓는 다른 방법 _ nn.Sequential 이용\n",
    "\n",
    "class CNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN2, self).__init__()\n",
    "        \n",
    "        self.feature_layer = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(8 * 8 * 16, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10)\n",
    "        \n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_layer(x)\n",
    "\n",
    "        x = x.view(-1, 8 * 8 * 16)\n",
    "        \n",
    "        x = self.fc_layer(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d942df4c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN2(\n",
      "  (feature_layer): Sequential(\n",
      "    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layer): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# nn.Sequential 이용\n",
    "# Optimizer, Objective Function 설정하기\n",
    "\n",
    "model = CNN2().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5efd5bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d5733d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()           # Optimizer의 Gradient를 초기화\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label) # CrossEntropy를 이용해 Loss값 계산\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd124433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()                          # 평가 상태로 지정\n",
    "    test_loss = 0\n",
    "    correct = 0                           # 올바른 Class로 평가했는지 알아보기 위해\n",
    "\n",
    "    with torch.no_grad():                 # Gradient를 통해 파라미터 값들이 업데이트되는 현상 방지\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()     # test_loss값 업데이트\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item() # 예측 클래스와 실제 레이블 클래스 비교\n",
    "    \n",
    "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)   # 정확도 계산\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b9908ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tTrain Loss: 2.312438\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tTrain Loss: 1.964561\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tTrain Loss: 1.918607\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tTrain Loss: 1.766003\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tTrain Loss: 1.620650\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tTrain Loss: 1.369281\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tTrain Loss: 1.576364\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tTrain Loss: 1.267878\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 1.4191, \tTest Accuracy: 48.25 % \n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tTrain Loss: 1.413111\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tTrain Loss: 1.483805\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tTrain Loss: 1.709003\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tTrain Loss: 1.305506\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tTrain Loss: 1.504539\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tTrain Loss: 1.249981\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tTrain Loss: 1.629030\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tTrain Loss: 1.303033\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 1.2829, \tTest Accuracy: 54.50 % \n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tTrain Loss: 1.221601\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tTrain Loss: 1.422018\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tTrain Loss: 1.181374\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tTrain Loss: 1.429436\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tTrain Loss: 0.977320\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tTrain Loss: 1.528760\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tTrain Loss: 1.218516\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tTrain Loss: 0.986515\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 1.2128, \tTest Accuracy: 57.14 % \n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tTrain Loss: 1.256087\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#log_soft + nllloss\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# 정의한 train함수 실행\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)     \u001b[38;5;66;03m# 각 Epoch별로 출력되는 Loss값과 Accuracy 값 계산\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[EPOCH: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m], \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m      7\u001b[0m         epoch, test_loss, test_accuracy))\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, log_interval)\u001b[0m\n\u001b[0;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m      7\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m----> 8\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m           \u001b[38;5;66;03m# Optimizer의 Gradient를 초기화\u001b[39;00m\n\u001b[0;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label) \u001b[38;5;66;03m# CrossEntropy를 이용해 Loss값 계산\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:222\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    220\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m foreach \u001b[38;5;129;01mor\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse):\n\u001b[1;32m--> 222\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     per_device_and_dtype_grads[p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdevice][p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype]\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39mgrad)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#log_soft + nllloss\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)   # 정의한 train함수 실행\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)     # 각 Epoch별로 출력되는 Loss값과 Accuracy 값 계산\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04de0f",
   "metadata": {},
   "source": [
    "# Test Loss: 1.0687, \tTest Accuracy: 63.02 % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d94a0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tTrain Loss: 2.350174\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tTrain Loss: 1.916218\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tTrain Loss: 1.745679\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tTrain Loss: 1.799946\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tTrain Loss: 1.620885\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tTrain Loss: 1.699204\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tTrain Loss: 1.344532\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tTrain Loss: 1.430129\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 1.4862, \tTest Accuracy: 46.21 % \n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tTrain Loss: 1.791456\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tTrain Loss: 1.197034\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tTrain Loss: 1.693008\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tTrain Loss: 1.860703\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tTrain Loss: 1.427170\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tTrain Loss: 1.468364\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tTrain Loss: 1.404573\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tTrain Loss: 1.495945\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 1.3449, \tTest Accuracy: 49.95 % \n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tTrain Loss: 1.425955\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tTrain Loss: 0.992561\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tTrain Loss: 1.695081\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tTrain Loss: 1.221802\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tTrain Loss: 1.320882\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tTrain Loss: 1.066949\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tTrain Loss: 1.277347\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tTrain Loss: 1.040246\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 1.2289, \tTest Accuracy: 55.41 % \n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tTrain Loss: 1.131951\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tTrain Loss: 1.299149\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tTrain Loss: 0.994775\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tTrain Loss: 1.302819\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tTrain Loss: 0.876397\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tTrain Loss: 1.344995\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tTrain Loss: 1.176748\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tTrain Loss: 1.285267\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 1.1513, \tTest Accuracy: 58.72 % \n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tTrain Loss: 0.872163\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tTrain Loss: 0.919109\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tTrain Loss: 1.085371\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tTrain Loss: 0.907122\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tTrain Loss: 1.114853\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tTrain Loss: 1.045584\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tTrain Loss: 1.117214\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tTrain Loss: 0.737929\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 1.1065, \tTest Accuracy: 60.70 % \n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tTrain Loss: 1.106278\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tTrain Loss: 1.069615\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tTrain Loss: 0.848393\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tTrain Loss: 1.195128\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tTrain Loss: 1.080494\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tTrain Loss: 1.260267\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tTrain Loss: 1.103289\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tTrain Loss: 1.024929\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 1.1022, \tTest Accuracy: 61.21 % \n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tTrain Loss: 1.042511\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tTrain Loss: 0.875150\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tTrain Loss: 1.023806\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tTrain Loss: 1.091284\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tTrain Loss: 0.789121\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tTrain Loss: 0.987120\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tTrain Loss: 0.859536\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tTrain Loss: 0.865784\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 1.1138, \tTest Accuracy: 60.61 % \n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tTrain Loss: 1.090782\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tTrain Loss: 0.729096\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tTrain Loss: 1.335327\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tTrain Loss: 1.044853\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tTrain Loss: 0.886340\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tTrain Loss: 0.770457\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tTrain Loss: 0.914422\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tTrain Loss: 0.704851\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 1.0383, \tTest Accuracy: 63.32 % \n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tTrain Loss: 0.810226\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tTrain Loss: 0.821485\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tTrain Loss: 1.059542\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tTrain Loss: 0.703869\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tTrain Loss: 1.059333\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tTrain Loss: 0.807378\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tTrain Loss: 0.708555\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tTrain Loss: 1.116577\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 1.0364, \tTest Accuracy: 64.00 % \n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tTrain Loss: 0.748474\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tTrain Loss: 0.827016\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tTrain Loss: 0.959950\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tTrain Loss: 0.628339\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tTrain Loss: 0.825740\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tTrain Loss: 1.231911\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tTrain Loss: 0.990449\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tTrain Loss: 0.940060\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 1.0128, \tTest Accuracy: 64.16 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X + crossentropy\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)   # 정의한 train함수 실행\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)     # 각 Epoch별로 출력되는 Loss값과 Accuracy 값 계산\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfceb90",
   "metadata": {},
   "source": [
    "# Test Loss: 1.0128, \tTest Accuracy: 64.16 % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bdcd214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-16503f65537c>:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tTrain Loss: 2.333009\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tTrain Loss: 2.076202\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tTrain Loss: 1.624784\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tTrain Loss: 2.140745\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tTrain Loss: 1.931095\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tTrain Loss: 1.710172\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tTrain Loss: 1.371194\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tTrain Loss: 1.400291\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 1.4459, \tTest Accuracy: 47.43 % \n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tTrain Loss: 1.612612\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tTrain Loss: 1.744985\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tTrain Loss: 1.355141\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tTrain Loss: 0.753993\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tTrain Loss: 0.937256\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tTrain Loss: 1.769789\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tTrain Loss: 1.153752\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tTrain Loss: 1.244892\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 1.3248, \tTest Accuracy: 52.50 % \n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tTrain Loss: 1.259078\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tTrain Loss: 1.105157\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tTrain Loss: 1.285842\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tTrain Loss: 1.207423\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tTrain Loss: 1.741844\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tTrain Loss: 1.254218\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tTrain Loss: 1.185040\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tTrain Loss: 1.091060\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 1.2001, \tTest Accuracy: 57.42 % \n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tTrain Loss: 1.168245\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tTrain Loss: 1.298603\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tTrain Loss: 1.556011\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tTrain Loss: 1.410072\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tTrain Loss: 1.299502\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tTrain Loss: 1.406017\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tTrain Loss: 1.253090\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tTrain Loss: 1.234713\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 1.1339, \tTest Accuracy: 59.67 % \n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tTrain Loss: 1.250995\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tTrain Loss: 1.650521\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tTrain Loss: 1.047101\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tTrain Loss: 0.893426\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tTrain Loss: 0.826502\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tTrain Loss: 1.283723\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tTrain Loss: 1.360038\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tTrain Loss: 1.009926\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 1.1125, \tTest Accuracy: 60.39 % \n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tTrain Loss: 1.064238\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tTrain Loss: 1.141084\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tTrain Loss: 1.045072\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tTrain Loss: 0.974604\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tTrain Loss: 0.832999\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tTrain Loss: 0.913503\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tTrain Loss: 1.176530\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tTrain Loss: 0.988384\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 1.1032, \tTest Accuracy: 61.28 % \n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tTrain Loss: 1.057127\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tTrain Loss: 1.049216\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tTrain Loss: 0.975942\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tTrain Loss: 0.969369\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tTrain Loss: 1.187827\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tTrain Loss: 0.984889\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tTrain Loss: 1.346962\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tTrain Loss: 0.887686\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 1.0953, \tTest Accuracy: 61.02 % \n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tTrain Loss: 0.778322\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tTrain Loss: 1.419294\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tTrain Loss: 0.729219\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tTrain Loss: 0.883544\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tTrain Loss: 1.157875\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tTrain Loss: 1.025048\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tTrain Loss: 1.161552\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tTrain Loss: 0.896570\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 1.0905, \tTest Accuracy: 61.87 % \n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tTrain Loss: 1.015196\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tTrain Loss: 0.978904\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tTrain Loss: 0.989902\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tTrain Loss: 1.086237\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tTrain Loss: 0.997937\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tTrain Loss: 0.910748\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tTrain Loss: 0.884551\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tTrain Loss: 1.404296\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 1.1082, \tTest Accuracy: 60.94 % \n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tTrain Loss: 0.846216\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tTrain Loss: 0.897690\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tTrain Loss: 0.826260\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tTrain Loss: 0.973368\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tTrain Loss: 1.324095\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tTrain Loss: 1.139499\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tTrain Loss: 1.055269\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tTrain Loss: 0.972550\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 1.0819, \tTest Accuracy: 61.93 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# log_entropy + crossentropy\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)   # 정의한 train함수 실행\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)     # 각 Epoch별로 출력되는 Loss값과 Accuracy 값 계산\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14552a03",
   "metadata": {},
   "source": [
    "# Test Loss: 1.0819, \tTest Accuracy: 61.93 % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28446b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1762ed",
   "metadata": {},
   "source": [
    "# log_soft + nllloss\n",
    "## Test Loss: 1.0687, \tTest Accuracy: 63.02 % \n",
    "\n",
    "  \n",
    "\n",
    "# X + crossentropy\n",
    "## Test Loss: 1.0128, \tTest Accuracy: 64.16 %\n",
    "\n",
    "  \n",
    "\n",
    "# log_entropy + crossentropy\n",
    "## Test Loss: 1.0819, \tTest Accuracy: 61.93 % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9445fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb264000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tTrain Loss: 2.301376\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tTrain Loss: 1.989344\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tTrain Loss: 1.743972\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tTrain Loss: 1.473368\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tTrain Loss: 1.869168\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tTrain Loss: 1.488392\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tTrain Loss: 1.576495\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tTrain Loss: 1.621193\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 1.4529, \tTest Accuracy: 47.10 % \n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tTrain Loss: 1.245101\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tTrain Loss: 1.455147\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tTrain Loss: 1.488896\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tTrain Loss: 1.498501\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tTrain Loss: 1.349244\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tTrain Loss: 1.437084\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tTrain Loss: 1.227584\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tTrain Loss: 1.697590\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 1.3259, \tTest Accuracy: 52.81 % \n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tTrain Loss: 1.702342\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tTrain Loss: 1.398966\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tTrain Loss: 1.251279\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tTrain Loss: 1.149536\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tTrain Loss: 1.368067\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tTrain Loss: 1.495910\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tTrain Loss: 1.304480\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tTrain Loss: 1.058435\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 1.2474, \tTest Accuracy: 55.21 % \n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tTrain Loss: 1.330998\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tTrain Loss: 1.240818\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tTrain Loss: 1.085621\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tTrain Loss: 1.092427\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tTrain Loss: 1.120162\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tTrain Loss: 1.471235\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tTrain Loss: 1.237676\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tTrain Loss: 1.166909\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 1.1672, \tTest Accuracy: 58.58 % \n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tTrain Loss: 1.515032\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tTrain Loss: 0.836662\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tTrain Loss: 1.213676\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tTrain Loss: 1.308527\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tTrain Loss: 1.308617\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tTrain Loss: 1.228271\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tTrain Loss: 1.166305\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tTrain Loss: 1.117735\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 1.1288, \tTest Accuracy: 60.00 % \n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tTrain Loss: 1.353382\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tTrain Loss: 0.998287\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tTrain Loss: 1.010849\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tTrain Loss: 0.906532\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tTrain Loss: 1.035872\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tTrain Loss: 0.835156\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tTrain Loss: 1.077421\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tTrain Loss: 1.077617\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 1.0942, \tTest Accuracy: 61.19 % \n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tTrain Loss: 0.552230\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tTrain Loss: 0.949028\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tTrain Loss: 0.994485\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tTrain Loss: 0.831526\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tTrain Loss: 1.193141\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tTrain Loss: 0.915831\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tTrain Loss: 1.040549\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tTrain Loss: 0.947821\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 1.0722, \tTest Accuracy: 62.10 % \n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tTrain Loss: 1.251362\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tTrain Loss: 0.921685\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tTrain Loss: 0.718239\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tTrain Loss: 0.953681\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tTrain Loss: 0.787154\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tTrain Loss: 0.872978\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tTrain Loss: 0.965006\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tTrain Loss: 0.911763\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 1.0679, \tTest Accuracy: 62.56 % \n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tTrain Loss: 0.830972\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tTrain Loss: 1.075356\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tTrain Loss: 0.926523\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tTrain Loss: 0.673216\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tTrain Loss: 0.788473\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tTrain Loss: 1.038148\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tTrain Loss: 0.940958\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tTrain Loss: 0.975216\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 1.0465, \tTest Accuracy: 63.23 % \n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tTrain Loss: 0.849167\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tTrain Loss: 0.865225\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tTrain Loss: 0.587653\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tTrain Loss: 0.900624\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tTrain Loss: 0.648326\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tTrain Loss: 0.716213\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tTrain Loss: 0.821026\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tTrain Loss: 0.700581\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 1.0531, \tTest Accuracy: 62.75 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CNN 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)   # 정의한 train함수 실행\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)     # 각 Epoch별로 출력되는 Loss값과 Accuracy 값 계산\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2352e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a944be",
   "metadata": {},
   "source": [
    "# Data Augmentation 기법 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ded780bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation가 적용된 CIFAR10 데이터 다운로드하기 (Train set, Test set 분리)\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root = \"../data/CIFAR_10\",\n",
    "                                train = True,\n",
    "                                download = True,\n",
    "                                transform = transforms.Compose([                          # 괄호 안 처리 과정을 거친 데이터를 불러오는 것\n",
    "                                    transforms.RandomHorizontalFlip(),                    # 해당 이미지를 50%확률로 좡우 반전하는 것\n",
    "                                    transforms.ToTensor(),                                # Tensor 형태로 변환\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5),                 # 다른 정규화 진행, RGB순으로 평균을 0.5씩 적용\n",
    "                                                         (0.5, 0.5, 0.5))                 # 정규화 진행시 이용하는 표준편차, RGB순으로 0.5씩 적용\n",
    "                                ]))\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root = \"../data/CIFAR_10\",\n",
    "                                train = False,\n",
    "                                transform = transforms.Compose([\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "                                ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                          shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                         batch_size = BATCH_SIZE,\n",
    "                                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca4e9860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 3, 32, 32]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce5aef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 설계하기\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "        in_channels = 3,\n",
    "        out_channels = 8,\n",
    "        kernel_size = 3,\n",
    "        padding = 1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "        in_channels = 8,\n",
    "        out_channels = 16,\n",
    "        kernel_size = 3,\n",
    "        padding = 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(\n",
    "        kernel_size = 2,\n",
    "        stride = 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * 16, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 8 * 8 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd6c6a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Optimizer, Objective Function 설정하기\n",
    "\n",
    "model = CNN().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "891275b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e804f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cfc9537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tTrain Loss: 2.292173\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tTrain Loss: 1.788858\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tTrain Loss: 1.694063\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tTrain Loss: 1.435321\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tTrain Loss: 1.367968\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tTrain Loss: 1.280787\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tTrain Loss: 1.424232\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tTrain Loss: 1.468520\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 1.3683, \tTest Accuracy: 49.83 % \n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tTrain Loss: 1.296031\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tTrain Loss: 1.220997\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tTrain Loss: 1.429867\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tTrain Loss: 1.164153\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tTrain Loss: 1.129965\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tTrain Loss: 1.190872\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tTrain Loss: 1.254067\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tTrain Loss: 1.118573\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 1.1694, \tTest Accuracy: 58.46 % \n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tTrain Loss: 1.052189\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tTrain Loss: 1.230509\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tTrain Loss: 1.021972\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tTrain Loss: 0.997464\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tTrain Loss: 1.059416\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tTrain Loss: 1.193898\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tTrain Loss: 1.178860\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tTrain Loss: 1.158223\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 1.0981, \tTest Accuracy: 61.43 % \n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tTrain Loss: 0.943926\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tTrain Loss: 1.049038\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tTrain Loss: 1.093771\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tTrain Loss: 1.136194\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tTrain Loss: 0.892992\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tTrain Loss: 0.983600\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tTrain Loss: 0.980298\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tTrain Loss: 1.097753\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 1.0424, \tTest Accuracy: 63.44 % \n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tTrain Loss: 1.010738\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tTrain Loss: 0.739687\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tTrain Loss: 0.811962\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tTrain Loss: 0.790561\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tTrain Loss: 0.753291\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tTrain Loss: 0.815797\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tTrain Loss: 0.820479\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tTrain Loss: 0.830833\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.9940, \tTest Accuracy: 64.45 % \n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tTrain Loss: 0.823669\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tTrain Loss: 0.998044\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tTrain Loss: 1.195914\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tTrain Loss: 0.828157\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tTrain Loss: 0.593410\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tTrain Loss: 0.985390\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tTrain Loss: 0.615062\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tTrain Loss: 1.070896\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.9814, \tTest Accuracy: 65.48 % \n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tTrain Loss: 0.878967\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tTrain Loss: 1.024942\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tTrain Loss: 0.691862\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tTrain Loss: 0.699323\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tTrain Loss: 0.727237\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tTrain Loss: 1.260456\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tTrain Loss: 0.795480\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tTrain Loss: 0.858611\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.9721, \tTest Accuracy: 65.77 % \n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tTrain Loss: 1.069357\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tTrain Loss: 1.208770\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tTrain Loss: 1.416356\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tTrain Loss: 1.228663\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tTrain Loss: 0.570558\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tTrain Loss: 0.870074\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tTrain Loss: 1.052096\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tTrain Loss: 0.975860\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.9390, \tTest Accuracy: 67.17 % \n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tTrain Loss: 0.704887\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tTrain Loss: 1.172029\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tTrain Loss: 0.863697\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tTrain Loss: 0.974644\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tTrain Loss: 1.023454\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tTrain Loss: 0.881258\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tTrain Loss: 0.774456\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tTrain Loss: 1.290738\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.9299, \tTest Accuracy: 67.44 % \n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tTrain Loss: 0.886679\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tTrain Loss: 0.723573\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tTrain Loss: 0.881426\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tTrain Loss: 0.928026\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tTrain Loss: 0.951812\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tTrain Loss: 0.565492\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tTrain Loss: 0.477501\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tTrain Loss: 0.711024\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.9456, \tTest Accuracy: 67.34 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CNN 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a14d1",
   "metadata": {},
   "source": [
    "### CNN 모델 결과  \n",
    "#### Test Loss: 1.0531, Test Accuracy: 62.75 %   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580b9f2",
   "metadata": {},
   "source": [
    "### Data Augmentation 기법 사용 결과  \n",
    "#### Test Loss: 0.9456, Test Accuracy: 67.34 %   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed4deda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
